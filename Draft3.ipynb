{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bb6e7d6",
   "metadata": {},
   "source": [
    "Layers of a CNN\n",
    "\n",
    "1. Input Layer - Shape = [batch_size, image_width, image_height, channels]\n",
    "\n",
    "batch_size - random sample from the original training set thats used during applying stochastic gradient descent.\n",
    "channels - number of color channels of the input images. This number could be 3 for RGB images or 1 for binary images.\n",
    "\n",
    "If the dataset is composed of monochrome 28x28 pixel images, then the desired shape for our input layer would be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eea6426",
   "metadata": {},
   "outputs": [],
   "source": [
    "[batch_size, 28, 28, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7b8647",
   "metadata": {},
   "source": [
    "To change the shape of the input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c84762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n",
    "\n",
    "# The batch size is denoted as -1, which means it will be determined dynamically based on the input data. \n",
    "# This allows us to fine-tune the CNN model by trying varying batch sizes during training or inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509234c8",
   "metadata": {},
   "source": [
    "2. Convolutional Step - The main purposeof these convolutional steps is to extract fetaures from the input images then feed them to a linear classifier. The whole idea of stacking convolutional steps is to be able to detect features anywhere in the image.\n",
    "\n",
    "If we wanted to apply 20 filters each of size 5x5 to the input layer with a ReLUactivation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721a51c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer_1 = tf.layers.conv2d(\n",
    "  inputs=input_layer,\n",
    "  filters=20,\n",
    "  kernel_size=[5, 5],\n",
    "  padding=\"same\",\n",
    "  activation=tf.nn.relu,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2f21c5",
   "metadata": {},
   "source": [
    "inputs - represents the input layer defined int he first step\n",
    "filters - specifies the number of filters to be applied to the input image. The higher the number of filters, the more features are extracted from the input image.\n",
    "kernel_size - represents the size of the filter/feature detector\n",
    "padding - we use 'same' here to introduce zero padding to the corner pixels of the input image\n",
    "activation - specifies the fuction to be used for the output of the convolutional opertation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3a4ac8",
   "metadata": {},
   "source": [
    "Introducing Non-Linearity\n",
    "\n",
    "We talked about feeding the output of the convolution step to an activation function, in this case, ReLU.\n",
    "The ReLU activation function replaces all negative pixel values with zero, this is done to introduce non linearity in the output image , as the data we are using is usually non-linear.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
